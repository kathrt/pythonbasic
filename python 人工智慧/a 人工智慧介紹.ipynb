{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15a5d6e",
   "metadata": {},
   "source": [
    "# 人工智慧\n",
    "----------------\n",
    "###### 主要概念：深度學習&rarr;主要技術為神經網路\n",
    "神經網路model\n",
    " - NN\n",
    " - CNN\n",
    " - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb1772",
   "metadata": {},
   "source": [
    "# AI 創建過程\n",
    "-------------------\n",
    " 1. 問問題\n",
    " 2. 數學建模\n",
    " 3. 準備資料：分為訓練資料和測試資料\n",
    " 4. 架構神經網路（NN, CNN, RNN, 強化學習, 生成對抗模式GAN, VAE）\n",
    " 5. 調整參數成為一個函數\n",
    " 6. 學習（訓練資料送進神經網路學習, 調整參數, 用loss funtion 觀察差值）\n",
    " ###### 基本上用gradient descent 神經網路的特性為 backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b2eca",
   "metadata": {},
   "source": [
    "# 標準NN\n",
    "--------------\n",
    " - Fully Connected Neural Networks(1980s就火紅的model)\n",
    " - 分為輸入層, 隱藏層, 輸出層&rarr; 隱藏層為三層以上就稱為深度學習\n",
    " - 每神經元接受若干個輸入, 然後送出一個輸出\n",
    " - 輸入變數&rarr;加權和(weights), 偏值(bias)\n",
    " - 最後加上激活函數&rarr;即為輸出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e569c",
   "metadata": {},
   "source": [
    "activation function\n",
    "1. ReLU\n",
    "2. Sigmoid\n",
    "3. Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41457120",
   "metadata": {},
   "source": [
    "# 神經網路訓練法\n",
    "-------------------------\n",
    " - 學習法：propagation\n",
    " - 固定結構神經網路的函數空間&rarr; F(theta)\n",
    " - 找到theta*(與目標函數最接近&rarr;loss funtion 最小)就是往局部極小值移動"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c472f",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "------------------------------\n",
    " - 梯度下降也就是多變數化為單變數&rarr;偏微分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02371368",
   "metadata": {},
   "source": [
    "# Feedforward Fully-Connected Neural Networks\n",
    "-------------------------------------------------------------------------\n",
    "###### 1. 手寫辨識&rarr;28X28矩陣=784維\n",
    "###### 1-1 One-Hot encoding &rarr; 3 = 0001000000 &rarr; 10維     \n",
    "###### 2. 利用soft max使全部數字機率加總為一\n",
    "###### 3. 準備訓練資料&rarr;手寫辨識 MNIST 有六萬筆訓練資料和一萬筆測試資料\n",
    "###### 4. 架構神經網路&rarr;輸入層為 784 維，輸出層為 10 維\n",
    "###### 5. 用 SDG (Stochastic Gradient Descent) 學習也就是 ' 打亂 ' 學習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a016d",
   "metadata": {},
   "source": [
    "# RNN\n",
    "-------------\n",
    "- Recurrent Neural Network\n",
    "- 有記憶的神經網路\n",
    "- 對話機器人：f(目前的字) = 下一個字 &rarr;翻譯, video captioning生成影片敘述, 生成一段文字, 畫一半的圖完成它\n",
    "- LSTM(Long Short Term Memory)\n",
    "- GRU(Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a008467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
